{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "pretrained_model = \"model_weights.pth\"\n",
    "use_cuda=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Vogt\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# MNIST Test dataset and dataloader declaration\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ])),\n",
    "        batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = Net()\n",
    "\n",
    "# Load the pretrained model\n",
    "model.load_state_dict(torch.load(pretrained_model))\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        # Send the data and label to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # Zero all existing gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Calculate gradients of model in backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect datagrad\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                #adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 9433 / 10000 = 0.9433\n",
      "Epsilon: 0.05\tTest Accuracy = 8769 / 10000 = 0.8769\n",
      "Epsilon: 0.1\tTest Accuracy = 7667 / 10000 = 0.7667\n",
      "Epsilon: 0.15\tTest Accuracy = 5813 / 10000 = 0.5813\n",
      "Epsilon: 0.2\tTest Accuracy = 3431 / 10000 = 0.3431\n",
      "Epsilon: 0.25\tTest Accuracy = 1498 / 10000 = 0.1498\n",
      "Epsilon: 0.3\tTest Accuracy = 578 / 10000 = 0.0578\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    " #Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "\n",
    "# epsilon = 0.3\n",
    "# acc, ex = test(model, device, test_loader, eps)\n",
    "# accuracies.append(acc)\n",
    "\n",
    "a, b, c, d, e, f, g = examples;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23d7f2449e8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#a = 0.0, b = 0.05, c = 0.1, d = 0.15, e = 0.2, f = 0.25, g = 0.3 \n",
    "label, new_label, image, = g[0]\n",
    "plt.title(\"{} -> {}\".format(label, new_label))\n",
    "plt.imshow(image, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7495678a5423b614518f1e7d37edbca4a8c36210f7349cfc3d660176cd54b7f2"
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
